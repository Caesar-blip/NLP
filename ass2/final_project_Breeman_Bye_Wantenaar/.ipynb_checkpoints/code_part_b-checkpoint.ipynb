{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3110bfd",
   "metadata": {},
   "source": [
    "# Intro to NLP: Assignment 2\n",
    "## Part B\n",
    "Nils Breeman, Sebastiaan Bye, Julius Wantenaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15edd6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sebas/nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sebas/nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchecklist\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchecklist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mperturb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Perturb\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchecklist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Editor\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\checklist\\perturb.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tenses\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m recursive_apply, MunchWithAdd\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\pattern\\text\\en\\__init__.py:61\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     INFINITIVE, PRESENT, PAST, FUTURE,\n\u001b[0;32m     55\u001b[0m     FIRST, SECOND, THIRD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     PARTICIPLE\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Import inflection functions.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minflect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     article, referenced, DEFINITE, INDEFINITE,\n\u001b[0;32m     63\u001b[0m     pluralize, singularize, NOUN, VERB, ADJECTIVE,\n\u001b[0;32m     64\u001b[0m     grade, comparative, superlative, COMPARATIVE, SUPERLATIVE,\n\u001b[0;32m     65\u001b[0m     verbs, conjugate, lemma, lexeme, tenses,\n\u001b[0;32m     66\u001b[0m     predicative, attributive\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Import quantification functions.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minflect_quantify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     70\u001b[0m     number, numerals, quantify, reflect\n\u001b[0;32m     71\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\pattern\\text\\en\\__init__.py:80\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Import all submodules.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inflect\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordlist\n\u001b[0;32m     83\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:74\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m     IC_MAX[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(IC_CORPUS[key]\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# This will hold the WordNet version\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m VERSION \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#---------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     78\u001b[0m DIACRITICS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124má\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mà\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124må\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mé\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mê\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mž\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m     89\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sebas/nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sebas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Packages\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import torch\n",
    "import joblib\n",
    "import checklist\n",
    "from checklist.perturb import Perturb\n",
    "import spacy\n",
    "from checklist.editor import Editor\n",
    "from simpletransformers.language_modeling import LanguageModelingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684c03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/olid-train.csv\")\n",
    "test = pd.read_csv(\"data/olid-test.csv\")\n",
    "diagnostics = pd.read_csv(\"data/olid-subset-diagnostic-tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d2ca69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/best_model.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs/best_model.sav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/best_model.sav'"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"outputs/best_model.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c18896",
   "metadata": {},
   "source": [
    "### Question 5. Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "diagnostics['perturbed'] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb005dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(diagnostics['text']):\n",
    "    diagnostics['perturbed'][i] = Perturb.add_typos(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(diagnostics['perturbed'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c951ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = diagnostics['labels']\n",
    "predicts = predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class 1')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=1)}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=1)}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=1)}')\n",
    "\n",
    "print()\n",
    "print('class 0')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=0)}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=0)}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=0)}')\n",
    "\n",
    "print()\n",
    "print('macro average')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "\n",
    "print()\n",
    "print('micro average')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=1, average='weighted')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04164cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_predicts = model.predict(diagnostics['text'].tolist())\n",
    "og_predicts = og_predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 1\n",
    "print(precision_score(gold, og_predicts, pos_label=1))\n",
    "print(recall_score(gold, og_predicts, pos_label=1))\n",
    "print(f1_score(gold, og_predicts, pos_label=1))\n",
    "\n",
    "\n",
    "# class 0\n",
    "print()\n",
    "print(precision_score(gold, og_predicts, pos_label=0))\n",
    "print(recall_score(gold, og_predicts, pos_label=0))\n",
    "print(f1_score(gold, og_predicts, pos_label=0))\n",
    "\n",
    "# macro average\n",
    "print()\n",
    "print(precision_score(gold, og_predicts, pos_label=1, average='macro'))\n",
    "print(recall_score(gold, og_predicts, pos_label=1, average='macro'))\n",
    "print(f1_score(gold, og_predicts, pos_label=1, average='macro'))\n",
    "\n",
    "\n",
    "# micro average\n",
    "print()\n",
    "print(precision_score(gold, og_predicts, pos_label=1, average='weighted'))\n",
    "print(recall_score(gold, og_predicts, pos_label=1, average='weighted'))\n",
    "print(f1_score(gold, og_predicts, pos_label=1, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(og_predicts)):\n",
    "    if og_predicts[i] == gold[i]:\n",
    "        if predicts[i] != gold[i]:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa82d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Failed to identify correctly\n",
    "'''\n",
    "print(diagnostics['perturbed'][11])\n",
    "print(diagnostics['perturbed'][16])\n",
    "print(diagnostics['perturbed'][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d164ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diagnostics['text'][11])\n",
    "print(diagnostics['text'][16])\n",
    "print(diagnostics['text'][34])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8338d4",
   "metadata": {},
   "source": [
    "### Question 6. Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca377164",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "diagnostics['negated'] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(diagnostics['text'].tolist()):\n",
    "    doc = nlp(sent)\n",
    "    try: \n",
    "        negated = Perturb.add_negation(doc)\n",
    "        if type(negated) == str:\n",
    "            diagnostics['negated'][i] = negated\n",
    "        else:\n",
    "            print(\"none found\")\n",
    "            diagnostics['negated'][i] = sent\n",
    "            print(i)\n",
    "    except:\n",
    "        diagnostics['negated'][i] = sent\n",
    "        print(\"didn't work boyyyy\")\n",
    "        print(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98943c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics['text'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics['negated'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(diagnostics['negated'].tolist())\n",
    "predicts = predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e433fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = diagnostics['labels']\n",
    "\n",
    "for i in range(len(predicts)):\n",
    "    if predicts[i] != gold[i]:\n",
    "        print(\"correct\")\n",
    "        print(diagnostics['labels'][i])\n",
    "        print(diagnostics['id'][i])\n",
    "        print(diagnostics['text'][i])\n",
    "        print(diagnostics['negated'][i])\n",
    "        print()\n",
    "    else:\n",
    "        print(\"incorrect\")\n",
    "        print(diagnostics['labels'][i])\n",
    "        print(diagnostics['id'][i])\n",
    "        print(diagnostics['text'][i])\n",
    "        print(diagnostics['negated'][i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72758e76",
   "metadata": {},
   "source": [
    "### Question 7. Creating negated masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = LanguageModelingModel(\"bert\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ecbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template('I hate {mask}.')\n",
    "data = ret.data\n",
    "gold = [1] * len(data)\n",
    "df['text'] = data\n",
    "df['gold'] = gold\n",
    "\n",
    "df2 = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template(r\"I don't hate {mask}.\")\n",
    "data = ret.data\n",
    "gold = [0] * len(data)\n",
    "df2['text'] = data\n",
    "df2['gold'] = gold\n",
    "df = df.append(df2)\n",
    "\n",
    "df2 = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template(r\"I don't hate {nationality} people.\")\n",
    "data = ret.data\n",
    "gold = [0] * len(data)\n",
    "df2['text'] = data\n",
    "df2['gold'] = gold\n",
    "df = df.append(df2)\n",
    "\n",
    "df2 = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template(r\"I hate {nationality} people.\")\n",
    "data = ret.data\n",
    "gold = [1] * len(data)\n",
    "df2['text'] = data\n",
    "df2['gold'] = gold\n",
    "df = df.append(df2)\n",
    "\n",
    "df2 = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template(r\"I don't hate {religion}.\")\n",
    "data = ret.data\n",
    "gold = [0] * len(data)\n",
    "df2['text'] = data\n",
    "df2['gold'] = gold\n",
    "df = df.append(df2)\n",
    "\n",
    "df2 = pd.DataFrame(columns = [\"text\", \"gold\"])\n",
    "ret = editor.template(r\"I hate {religion}.\")\n",
    "data = ret.data\n",
    "gold = [1] * len(data)\n",
    "df2['text'] = data\n",
    "df2['gold'] = gold\n",
    "df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(df['text'].tolist())\n",
    "predicts = predicts[0]\n",
    "gold = df['gold'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i, predict in enumerate(predicts):\n",
    "    if predict == gold[i]:\n",
    "        count+=1\n",
    "        print(\"correct\")\n",
    "        print(df['text'][i])\n",
    "        print(predicts[i])\n",
    "        print()\n",
    "    else:\n",
    "        print(\"incorrect\")\n",
    "        print(df['text'][i])\n",
    "        print(predicts[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7701e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
