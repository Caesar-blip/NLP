{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2988b7be",
   "metadata": {},
   "source": [
    "# Intro to NLP: Assignment 2\n",
    "## Part A\n",
    "Nils Breeman, Sebastiaan Bye, Julius Wantenaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8868f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import torch\n",
    "from sklearn.metrics import (f1_score, recall_score, precision_score)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a94016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b957bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/sebas/Documents/GitHub/NLP/ass2/data/olid-train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/sebas/Documents/GitHub/NLP/ass2/data/olid-test.csv\")\n",
    "diagnostics = pd.read_csv(\"C:/Users/sebas/Documents/GitHub/NLP/ass2/data/olid-subset-diagnostic-tests.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2ca36",
   "metadata": {},
   "source": [
    "### Question 1. Class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0047d3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instance with class label 1: 4400\n",
      "Relative label frequency with class label 1: 0.3323262839879154\n",
      "Example sentence: @USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"\n",
      "\n",
      "Number of instance with class label 2: 8840\n",
      "Relative label frequency with class label 2: 0.6676737160120846\n",
      "Example sentence: Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instance with class label 1: {len(train.loc[train['labels'] == 1])}\")\n",
    "print(f\"Relative label frequency with class label 1: {len(train.loc[train['labels'] == 1]) / len(train)}\")\n",
    "print(f\"Example sentence: {train.loc[train['labels'] == 1]['text'][3]}\")\n",
    "print()\n",
    "print(f\"Number of instance with class label 2: {len(train.loc[train['labels'] == 0])}\")\n",
    "print(f\"Relative label frequency with class label 2: {len(train.loc[train['labels'] == 0]) / len(train)}\")\n",
    "print(f\"Example sentence: {train.loc[train['labels'] == 0]['text'][2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff12043",
   "metadata": {},
   "source": [
    "### Question 2. Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d713449",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = test.shape[0]\n",
    "predicts = np.random.randint(0,2, rows)\n",
    "gold = test['labels']\n",
    "\n",
    "one_recall = []\n",
    "one_precision = []\n",
    "one_f1 = []\n",
    "\n",
    "zero_recall = []\n",
    "zero_precision = []\n",
    "zero_f1 = []\n",
    "\n",
    "macro_recall = []\n",
    "macro_precision = []\n",
    "macro_f1 = []\n",
    "\n",
    "micro_recall = []\n",
    "micro_precision = []\n",
    "micro_f1 = []\n",
    "for i in range(1000):\n",
    "    # class 1\n",
    "    one_recall.append(recall_score(gold, predicts, pos_label=1))\n",
    "    one_precision.append(precision_score(gold, predicts, pos_label=1))\n",
    "    one_f1.append(f1_score(gold, predicts, pos_label=1))\n",
    "\n",
    "\n",
    "    # class 0\n",
    "    zero_recall.append(recall_score(gold, predicts, pos_label=0))\n",
    "    zero_precision.append(precision_score(gold, predicts, pos_label=0))\n",
    "    zero_f1.append(f1_score(gold, predicts, pos_label=0))\n",
    "\n",
    "\n",
    "    # macro average\n",
    "    one_recall.append(recall_score(gold, predicts, pos_label=1))\n",
    "    one_precision.append(precision_score(gold, predicts, pos_label=1))\n",
    "    one_f1.append(f1_score(gold, predicts, pos_label=1))\n",
    "    \n",
    "    \n",
    "    macro_recall.append(recall_score(gold, predicts, pos_label=1, average='macro'))\n",
    "    macro_precision.append(precision_score(gold, predicts, pos_label=1, average='macro'))\n",
    "    macro_f1.append(f1_score(gold, predicts, pos_label=1, average='macro'))\n",
    "\n",
    "\n",
    "    # micro average\n",
    "    micro_recall.append(recall_score(gold, predicts, pos_label=1, average='weighted'))\n",
    "    micro_precision.append(precision_score(gold, predicts, pos_label=1, average='weighted'))\n",
    "    micro_f1.append(f1_score(gold, predicts, pos_label=1, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c32874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Baseline\")\n",
    "print('Class 1')\n",
    "print(f'Precision: {np.mean(one_precision)}')\n",
    "print(f'Recall: {np.mean(one_recall)}')\n",
    "print(f'F1: {np.mean(one_f1)})\n",
    "\n",
    "print()\n",
    "print('Class 0')\n",
    "print(f'Precision {np.mean(zero_precision)}')\n",
    "print(f'Recall {np.mean(zero_recall)}')\n",
    "print(f'F1{np.mean(zero_f1)}')\n",
    "\n",
    "print()\n",
    "print('Macro-average')\n",
    "print(f'Precision {np.mean(macro_precision)}')\n",
    "print(f'Recall {np.mean(macro_recall)}')\n",
    "print(f'F1 {np.mean(macro_f1)}')\n",
    "\n",
    "print('Weighted average')\n",
    "print(f'Precision {np.mean(micro_precision)}')\n",
    "print(f'Recall {np.mean(micro_recall)}')\n",
    "print(f'F1 {np.mean(micro_f1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b859e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Majority\n",
    "predicts = np.zeros(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class 1')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=1)}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=1)}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=1)}')\n",
    "\n",
    "print()\n",
    "print('class 0')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=0)}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=0)}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=0)}')\n",
    "\n",
    "print()\n",
    "print('macro average')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "\n",
    "\n",
    "print()\n",
    "print('micro average')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=1, average='weighted')}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea8387",
   "metadata": {},
   "source": [
    "### Question 3. Classification by fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(\"bert\", \"bert-base-cased\", use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfe54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"outputs/best_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd19d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test['text'].tolist())\n",
    "predicts = predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d406e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = test['labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class 1')\n",
    "print(f'precision {precision_score(gold, predicts, pos_label=1)}')\n",
    "print(f'recall {recall_score(gold, predicts, pos_label=1)}')\n",
    "print(f'f1 score {f1_score(gold, predicts, pos_label=1)}')\n",
    "\n",
    "print()\n",
    "print('class 0')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=0)}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=0)}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=0)}')\n",
    "\n",
    "print()\n",
    "print('macro average')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=1, average='macro')}')\n",
    "\n",
    "print()\n",
    "print('micro average')\n",
    "print(f'Precision {precision_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'Recall {recall_score(gold, predicts, pos_label=1, average='weighted')}')\n",
    "print(f'F1 score {f1_score(gold, predicts, pos_label=1, average='weighted')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(  gold, predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd853bf7",
   "metadata": {},
   "source": [
    "### Question 4. Inspect the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part a\n",
    "'''\n",
    "\n",
    "one_string = \" \".join(train['text'].tolist())\n",
    "tokens = one_string.split(\" \")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_split = model.tokenizer.tokenize(one_string)\n",
    "print(f\"Number of words split: {len(set(tokens) - set(after_split))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. \n",
    "'''\n",
    "print(f\"Average number {(len(after_split) - len(tokens)) / len(set(tokens) - set(after_split))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb70813",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Not meaningful: Pick manually not meaningful\n",
    "'''\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7432ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d. Longest subword\n",
    "'''\n",
    "\n",
    "length = 0\n",
    "word=\"\"\n",
    "for key in model.tokenizer.vocab.keys():\n",
    "    if len(key) > 2:\n",
    "        if key[0] == \"#\" and key[1] == \"#\":\n",
    "            if len(key) > length:\n",
    "                length=len(key)\n",
    "                word = key\n",
    "                \n",
    "print(length)\n",
    "print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
