{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95e0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a9f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "with open (\"data/preprocessed/train/sentences.txt\", encoding = \"utf8\") as text:\n",
    "    \n",
    "    data = text.readlines()\n",
    "    \n",
    "#     clean_data = []\n",
    "#     for i in range(len(data)):\n",
    "#         clean_data.append(str(data[i]).replace(\"\\\"\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\"))\n",
    "    \n",
    "    string = \"\"\n",
    "    for i in range(len(data)):\n",
    "#         string += str(data[i]).replace(\"\\\"\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "        string += str(data[i]).replace(\"\\n\", \" \").replace(\"\\\\\", \"\")\n",
    "#         string += str(data[i]).replace(\"\\n\", \" \")\n",
    "        \n",
    "    \n",
    "    doc = nlp(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af604246",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2320f9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 15209\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokens\n",
    "'''\n",
    "count = 0\n",
    "for token in doc:\n",
    "    count+=1\n",
    "\n",
    "print(f\"Number of tokens {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05852c",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3249b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of types 3744\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Types\n",
    "'''\n",
    "count = 0\n",
    "unique = []\n",
    "\n",
    "for words in doc:\n",
    "\n",
    "    unique.append(words.text)\n",
    "    \n",
    "unique = np.unique(unique)    \n",
    "\n",
    "print(f\"The number of types {len(unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d57fb",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301b95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15209 13242 3721\n"
     ]
    }
   ],
   "source": [
    "# NOTE: method from practice notebook\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "    word_frequencies.update(words)\n",
    "    \n",
    "#print(word_frequencies)\n",
    "\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "\n",
    "print(num_tokens, num_words, num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3776e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13242\n"
     ]
    }
   ],
   "source": [
    "# skip = [\"PUNCT\", \"SYM\", \"X\", \"SPACE\", \"NUM\"]\n",
    "skip = [\"PUNCT\"]\n",
    "word_list = []\n",
    "for token in doc:\n",
    "        if not token.is_punct:\n",
    "#     if token.pos_ not in skip:\n",
    "            word_list.append(token.text)\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af7b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words 13242\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of words {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e618fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12707\n"
     ]
    }
   ],
   "source": [
    "# OLD METHOD\n",
    "skip = [\"PUNCT\", \"SYM\", \"X\", \"SPACE\", \"NUM\"]\n",
    "word_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in skip:\n",
    "        if token.lemma_.isalpha():\n",
    "            word_list.append(token.text)\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a548d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words 12707\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of words {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d71c53",
   "metadata": {},
   "source": [
    "### Average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f1ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average words per sentence is 19.083457526080476\n"
     ]
    }
   ],
   "source": [
    "worcount = len(word_list)\n",
    "sentcount = len(list(doc.sents))\n",
    "print(f\"The average words per sentence is {wordcount/sentcount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68245c7e",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201c6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = []\n",
    "\n",
    "for i in word_list:\n",
    "    word_lengths.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2207a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average word length is 4.973007004013536 +/- 2.585102110898612\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average word length is {np.mean(word_lengths)} +/- {np.std(word_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55d493",
   "metadata": {},
   "source": [
    "## 2. Word Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c211b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.tag_\n",
    "         for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f78f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokens)\n",
    "tokens = np.unique(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcd0b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = dict.fromkeys(tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f073a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    token_count[token.tag_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "872478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = sorted(token_count, key=token_count.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49c4e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'NNP', 'IN', 'DT', 'JJ', 'NNS', ',', 'VBD', '.', 'VBN']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81026e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberdict = dict.fromkeys(my_keys, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a146583",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_keys:\n",
    "    \n",
    "    # get universal POS applicable to finegrained POS\n",
    "    uniPOS = [token.pos_\n",
    "             for token in doc if token.tag_ == i]\n",
    "    uniPOS = list(set(uniPOS))\n",
    "    \n",
    "    # get all words\n",
    "    all_words = [token.text\n",
    "             for token in doc if token.tag_ ==i]\n",
    "    \n",
    "    # get count of words for finegrained POS token\n",
    "    fineg_count = len(all_words)\n",
    "    \n",
    "    # create dict with all words existing for this finegrained POS token\n",
    "    words = np.array(all_words)\n",
    "    words = np.unique(words)\n",
    "    word_dict = dict.fromkeys(words, 0)\n",
    "    \n",
    "    # count number of occurences of word\n",
    "    for tok in all_words:\n",
    "        word_dict[tok] += 1\n",
    "    \n",
    "    # get most common and least common words\n",
    "    keys_freqtokens = sorted(word_dict, key=word_dict.get, reverse=True)[:3]\n",
    "    keys_unfreqtokens = sorted(word_dict, key=word_dict.get)[:1]\n",
    "    \n",
    "    uberdict[i] = [uniPOS, fineg_count, keys_freqtokens, keys_unfreqtokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6efa588f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': [['PRON', 'NOUN'], 2055, ['year', 'report', 'time'], ['A.']],\n",
       " 'NNP': [['AUX', 'PROPN'], 1793, ['US', 'President', 'U.S.'], ['-']],\n",
       " 'IN': [['ADP', 'SCONJ'], 1744, ['of', 'in', 'to'], ['About']],\n",
       " 'DT': [['PRON', 'DET'], 1379, ['the', 'a', 'The'], ['An']],\n",
       " 'JJ': [['ADJ'], 872, ['other', 'Russian', 'presidential'], ['21st']],\n",
       " 'NNS': [['NOUN'], 781, ['ants', 'troops', 'people'], ['1970s']],\n",
       " ',': [['PUNCT'], 699, [',', ';', '…'], [';']],\n",
       " 'VBD': [['AUX', 'VERB'], 658, ['was', 'were', 'said'], ['acknowledged']],\n",
       " '.': [['PUNCT'], 655, ['.', '?', '!'], ['!']],\n",
       " 'VBN': [['AUX', 'VERB'], 501, ['been', 'accused', 'killed'], ['-']]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a375e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tags = 0\n",
    "for i in uberdict.values():\n",
    "    total_tags += i[1]\n",
    "\n",
    "for i in uberdict.keys():\n",
    "    uberdict[i].append(uberdict[i][1]/total_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "680411a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#uberdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36d39099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(uberdict,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee1de618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>[PRON, NOUN]</td>\n",
       "      <td>2055</td>\n",
       "      <td>[year, report, time]</td>\n",
       "      <td>[A.]</td>\n",
       "      <td>0.184520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>[AUX, PROPN]</td>\n",
       "      <td>1793</td>\n",
       "      <td>[US, President, U.S.]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0.160995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>[ADP, SCONJ]</td>\n",
       "      <td>1744</td>\n",
       "      <td>[of, in, to]</td>\n",
       "      <td>[About]</td>\n",
       "      <td>0.156595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>[PRON, DET]</td>\n",
       "      <td>1379</td>\n",
       "      <td>[the, a, The]</td>\n",
       "      <td>[An]</td>\n",
       "      <td>0.123821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>[ADJ]</td>\n",
       "      <td>872</td>\n",
       "      <td>[other, Russian, presidential]</td>\n",
       "      <td>[21st]</td>\n",
       "      <td>0.078298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>781</td>\n",
       "      <td>[ants, troops, people]</td>\n",
       "      <td>[1970s]</td>\n",
       "      <td>0.070127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>[PUNCT]</td>\n",
       "      <td>699</td>\n",
       "      <td>[,, ;, …]</td>\n",
       "      <td>[;]</td>\n",
       "      <td>0.062764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>[AUX, VERB]</td>\n",
       "      <td>658</td>\n",
       "      <td>[was, were, said]</td>\n",
       "      <td>[acknowledged]</td>\n",
       "      <td>0.059082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>[PUNCT]</td>\n",
       "      <td>655</td>\n",
       "      <td>[., ?, !]</td>\n",
       "      <td>[!]</td>\n",
       "      <td>0.058813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>[AUX, VERB]</td>\n",
       "      <td>501</td>\n",
       "      <td>[been, accused, killed]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0.044985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0     1                               2               3  \\\n",
       "NN   [PRON, NOUN]  2055            [year, report, time]            [A.]   \n",
       "NNP  [AUX, PROPN]  1793           [US, President, U.S.]             [-]   \n",
       "IN   [ADP, SCONJ]  1744                    [of, in, to]         [About]   \n",
       "DT    [PRON, DET]  1379                   [the, a, The]            [An]   \n",
       "JJ          [ADJ]   872  [other, Russian, presidential]          [21st]   \n",
       "NNS        [NOUN]   781          [ants, troops, people]         [1970s]   \n",
       ",         [PUNCT]   699                       [,, ;, …]             [;]   \n",
       "VBD   [AUX, VERB]   658               [was, were, said]  [acknowledged]   \n",
       ".         [PUNCT]   655                       [., ?, !]             [!]   \n",
       "VBN   [AUX, VERB]   501         [been, accused, killed]             [-]   \n",
       "\n",
       "            4  \n",
       "NN   0.184520  \n",
       "NNP  0.160995  \n",
       "IN   0.156595  \n",
       "DT   0.123821  \n",
       "JJ   0.078298  \n",
       "NNS  0.070127  \n",
       ",    0.062764  \n",
       "VBD  0.059082  \n",
       ".    0.058813  \n",
       "VBN  0.044985  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90655fb",
   "metadata": {},
   "source": [
    "## 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f142a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_\n",
    "         for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a44c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba2bd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = {}\n",
    "lemma_set = []\n",
    "token_set = []\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "for i in range(len(sentences)):\n",
    "    for token in sentences[i]:\n",
    "        if not token.is_punct:\n",
    "            if token.lemma_ not in lemmas.keys():\n",
    "                lemmas[token.lemma_] = {}\n",
    "                lemmas[token.lemma_][token.text] = [i]\n",
    "                lemma_set.append(token.lemma_)\n",
    "                token_set.append(token.text)\n",
    "            elif token.text not in lemmas[token.lemma_].keys():\n",
    "                lemmas[token.lemma_][token.text] = [i]\n",
    "                token_set.append(token.text)\n",
    "            elif token.text in lemmas[token.lemma_].keys() and token.lemma_ in lemmas.keys():\n",
    "                lemmas[token.lemma_][token.text] += [i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "066512b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "say\n",
      "report\n",
      "start\n",
      "do\n",
      "early\n",
      "challenge\n",
      "fall\n",
      "move\n",
      "fly\n",
      "call\n",
      "use\n",
      "close\n",
      "appear\n",
      "find\n",
      "go\n",
      "seek\n",
      "kill\n",
      "measure\n",
      "claim\n",
      "result\n",
      "man\n",
      "include\n",
      "give\n",
      "respond\n",
      "name\n",
      "see\n",
      "add\n",
      "make\n",
      "run\n",
      "take\n",
      "remain\n",
      "election\n",
      "average\n"
     ]
    }
   ],
   "source": [
    "for k in lemmas.keys():\n",
    "    if len(lemmas[k].keys()) > 3:\n",
    "        print(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dbb416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['challenged', 'challenging', 'challenges', 'challenge'])\n",
      "\n",
      "That decision was the one challenged unsuccessfully in the High Court .\n",
      "\n",
      "Next week , on January 24 , the Supreme Court is due to deliver a decision in a case challenging the government 's right to issue Article 50 — which starts the Brexit negotiations — without the consultation of Parliament .\n",
      "\n",
      "U.S. presidential candidate Mark Everson challenges debate exclusion U.S. Republican Party presidential candidate Mark Everson , former commissioner of the Internal Revenue Service ( IRS ) , filed a complaint on Monday with the Federal Election Commission ( FEC ) to challenge his exclusion from Thursday 's first Fox News Republican Party presidential debate .\n",
      "\n",
      "Election law expert Richard Winger , publisher of Ballot Access News , says Everson is \" completely correct \" in his challenge .\n"
     ]
    }
   ],
   "source": [
    "print(lemmas['challenge'].keys())\n",
    "print()\n",
    "lem1tok1 = lemmas['challenge']['challenged'][0]\n",
    "lem1tok2 = lemmas['challenge']['challenging'][0]\n",
    "lem1tok3 = lemmas['challenge']['challenges'][0]\n",
    "lem1tok4 = lemmas['challenge']['challenge'][1]\n",
    "print(sentences[lem1tok1])\n",
    "print()\n",
    "print(sentences[lem1tok2])\n",
    "print()\n",
    "print(sentences[lem1tok3])\n",
    "print()\n",
    "print(sentences[lem1tok4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cf509",
   "metadata": {},
   "source": [
    "### Part 5. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f28e46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence object and an array for entity information\n",
    "doc_sent = doc.sents\n",
    "array = np.zeros(len(list(doc.sents))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "840b6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over sentences save entities in array\n",
    "counter = 0\n",
    "first_five = []\n",
    "for sent in doc.sents:\n",
    "    first_five.append(sent)\n",
    "    temp_entity = []\n",
    "    sent_text = nlp(str(sent))\n",
    "    \n",
    "    for ent in sent_text.ents:\n",
    "        temp_entity.append([ent.text, ent.label_])\n",
    "    \n",
    "    array[counter] = temp_entity\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c129e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of named entities: 2108\n",
      "Number of different entity labels: 17\n"
     ]
    }
   ],
   "source": [
    "# Get entities and entity labels back from array\n",
    "named_entities = 0\n",
    "entity_labels = []\n",
    "\n",
    "for i in range(len(array)):\n",
    "    \n",
    "    temp_array = np.array(array[i]).T\n",
    "    \n",
    "    if temp_array.size != 0:\n",
    "        named_entities += len(temp_array[0]) + 1\n",
    "        entity_labels.append(temp_array[1].tolist())\n",
    "    \n",
    "print(f\"Number of named entities: {named_entities}\")    \n",
    "print(f\"Number of different entity labels: {len(set(np.hstack(entity_labels).tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5bc3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "children are thought to be aged three , eight , and ten years , alongside an eighteen-month-old baby .\n",
      "Entities: ['three , eight' 'ten years' 'eighteen-month-old']\n",
      "Labels: ['DATE' 'DATE' 'DATE']\n",
      "\n",
      "Sentence 2:\n",
      "We mixed different concentrations of ROS with the spores , plated them out on petridishes with an agar-solution where fungus can grow on .\n",
      "Entities: ['ROS']\n",
      "Labels: ['GPE']\n",
      "\n",
      "Sentence 3:\n",
      "They feel they are under-represented in higher education and are suffering in a regional economic downturn .\n",
      "\n",
      "Sentence 4:\n",
      "Especially as it concerns a third party building up its military presence near our borders .\n",
      "Entities: ['third']\n",
      "Labels: ['ORDINAL']\n",
      "\n",
      "Sentence 5:\n",
      "Police said three children were hospitalised for \" severe dehydration \" .\n",
      "Entities: ['three']\n",
      "Labels: ['CARDINAL']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse first five sentences\n",
    "for i in range(0,5):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(first_five[i])\n",
    "    if np.array(array[i]).T.size != 0:\n",
    "        print(f\"Entities: {np.array(array[i]).T[0]}\")\n",
    "        print(f\"Labels: {np.array(array[i]).T[1]}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c78bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
