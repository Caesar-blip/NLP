{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95e0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a9f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "with open (\"data/preprocessed/train/sentences.txt\", encoding = \"utf8\") as text:\n",
    "    \n",
    "    data = text.readlines()\n",
    "    \n",
    "#     clean_data = []\n",
    "#     for i in range(len(data)):\n",
    "#         clean_data.append(str(data[i]).replace(\"\\\"\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\"))\n",
    "    \n",
    "    string = \"\"\n",
    "    for i in range(len(data)):\n",
    "#         string += str(data[i]).replace(\"\\\"\", \"\").replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "        string += str(data[i]).replace(\"\\n\", \" \").replace(\"\\\\\", \"\")\n",
    "#         string += str(data[i]).replace(\"\\n\", \" \")\n",
    "        \n",
    "    \n",
    "    doc = nlp(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af604246",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2320f9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 15209\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokens\n",
    "'''\n",
    "count = 0\n",
    "for token in doc:\n",
    "    count+=1\n",
    "\n",
    "print(f\"Number of tokens {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05852c",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3249b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of types 3744\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Types\n",
    "'''\n",
    "count = 0\n",
    "unique = []\n",
    "\n",
    "for words in doc:\n",
    "\n",
    "    unique.append(words.text)\n",
    "    \n",
    "unique = np.unique(unique)    \n",
    "\n",
    "print(f\"The number of types {len(unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d57fb",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301b95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15209 13242 3721\n"
     ]
    }
   ],
   "source": [
    "# NOTE: method from practice notebook\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "    word_frequencies.update(words)\n",
    "    \n",
    "#print(word_frequencies)\n",
    "\n",
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "\n",
    "print(num_tokens, num_words, num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3776e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13242\n"
     ]
    }
   ],
   "source": [
    "# skip = [\"PUNCT\", \"SYM\", \"X\", \"SPACE\", \"NUM\"]\n",
    "skip = [\"PUNCT\"]\n",
    "word_list = []\n",
    "for token in doc:\n",
    "        if not token.is_punct:\n",
    "#     if token.pos_ not in skip:\n",
    "            word_list.append(token.text)\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af7b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words 13242\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of words {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e618fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12707\n"
     ]
    }
   ],
   "source": [
    "# OLD METHOD\n",
    "skip = [\"PUNCT\", \"SYM\", \"X\", \"SPACE\", \"NUM\"]\n",
    "word_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in skip:\n",
    "        if token.lemma_.isalpha():\n",
    "            word_list.append(token.text)\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a548d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words 12707\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of words {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d71c53",
   "metadata": {},
   "source": [
    "### Average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f1ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average words per sentence is 18.937406855439644\n"
     ]
    }
   ],
   "source": [
    "wordcount = len(word_list)\n",
    "sentcount = len(list(doc.sents))\n",
    "print(f\"The average words per sentence is {wordcount/sentcount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68245c7e",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201c6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = []\n",
    "\n",
    "for i in word_list:\n",
    "    word_lengths.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2207a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average word length is 4.973007004013536 +/- 2.585102110898612\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average word length is {np.mean(word_lengths)} +/- {np.std(word_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55d493",
   "metadata": {},
   "source": [
    "## 2. Word Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c211b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.tag_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokens)\n",
    "tokens = np.unique(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd0b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = dict.fromkeys(tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90f073a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    token_count[token.tag_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "872478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = sorted(token_count, key=token_count.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e49c4e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'NNP', 'IN', 'DT', 'JJ', 'NNS', ',', 'VBD', '.', 'VBN']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81026e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberdict = dict.fromkeys(my_keys, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a146583",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in my_keys:\n",
    "    \n",
    "    # get universal POS applicable to finegrained POS\n",
    "    uniPOS = [token.pos_\n",
    "             for token in doc if token.tag_ == i]\n",
    "    uniPOS = list(set(uniPOS))\n",
    "    \n",
    "    # get all words\n",
    "    all_words = [token.text\n",
    "             for token in doc if token.tag_ ==i]\n",
    "    \n",
    "    # get count of words for finegrained POS token\n",
    "    fineg_count = len(all_words)\n",
    "    \n",
    "    # create dict with all words existing for this finegrained POS token\n",
    "    words = np.array(all_words)\n",
    "    words = np.unique(words)\n",
    "    word_dict = dict.fromkeys(words, 0)\n",
    "    \n",
    "    # count number of occurences of word\n",
    "    for tok in all_words:\n",
    "        word_dict[tok] += 1\n",
    "    \n",
    "    # get most common and least common words\n",
    "    keys_freqtokens = sorted(word_dict, key=word_dict.get, reverse=True)[:3]\n",
    "    keys_unfreqtokens = sorted(word_dict, key=word_dict.get)[:1]\n",
    "    \n",
    "    uberdict[i] = [uniPOS, fineg_count, keys_freqtokens, keys_unfreqtokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6efa588f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': [['PRON', 'NOUN'], 2055, ['year', 'report', 'time'], ['A.']],\n",
       " 'NNP': [['PROPN', 'AUX'], 1793, ['US', 'President', 'U.S.'], ['-']],\n",
       " 'IN': [['ADP', 'SCONJ'], 1744, ['of', 'in', 'to'], ['About']],\n",
       " 'DT': [['PRON', 'DET'], 1379, ['the', 'a', 'The'], ['An']],\n",
       " 'JJ': [['ADJ'], 872, ['other', 'Russian', 'presidential'], ['21st']],\n",
       " 'NNS': [['NOUN'], 781, ['ants', 'troops', 'people'], ['1970s']],\n",
       " ',': [['PUNCT'], 699, [',', ';', '…'], [';']],\n",
       " 'VBD': [['VERB', 'AUX'], 658, ['was', 'were', 'said'], ['acknowledged']],\n",
       " '.': [['PUNCT'], 655, ['.', '?', '!'], ['!']],\n",
       " 'VBN': [['VERB', 'AUX'], 501, ['been', 'accused', 'killed'], ['-']]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a375e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tags = 0\n",
    "for i in uberdict.values():\n",
    "    total_tags += i[1]\n",
    "\n",
    "for i in uberdict.keys():\n",
    "    uberdict[i].append(uberdict[i][1]/total_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "680411a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#uberdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36d39099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(uberdict,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee1de618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>[PRON, NOUN]</td>\n",
       "      <td>2055</td>\n",
       "      <td>[year, report, time]</td>\n",
       "      <td>[A.]</td>\n",
       "      <td>0.184520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>[PROPN, AUX]</td>\n",
       "      <td>1793</td>\n",
       "      <td>[US, President, U.S.]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0.160995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>[ADP, SCONJ]</td>\n",
       "      <td>1744</td>\n",
       "      <td>[of, in, to]</td>\n",
       "      <td>[About]</td>\n",
       "      <td>0.156595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>[PRON, DET]</td>\n",
       "      <td>1379</td>\n",
       "      <td>[the, a, The]</td>\n",
       "      <td>[An]</td>\n",
       "      <td>0.123821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>[ADJ]</td>\n",
       "      <td>872</td>\n",
       "      <td>[other, Russian, presidential]</td>\n",
       "      <td>[21st]</td>\n",
       "      <td>0.078298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>781</td>\n",
       "      <td>[ants, troops, people]</td>\n",
       "      <td>[1970s]</td>\n",
       "      <td>0.070127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>[PUNCT]</td>\n",
       "      <td>699</td>\n",
       "      <td>[,, ;, …]</td>\n",
       "      <td>[;]</td>\n",
       "      <td>0.062764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>[VERB, AUX]</td>\n",
       "      <td>658</td>\n",
       "      <td>[was, were, said]</td>\n",
       "      <td>[acknowledged]</td>\n",
       "      <td>0.059082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>[PUNCT]</td>\n",
       "      <td>655</td>\n",
       "      <td>[., ?, !]</td>\n",
       "      <td>[!]</td>\n",
       "      <td>0.058813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>[VERB, AUX]</td>\n",
       "      <td>501</td>\n",
       "      <td>[been, accused, killed]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0.044985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0     1                               2               3  \\\n",
       "NN   [PRON, NOUN]  2055            [year, report, time]            [A.]   \n",
       "NNP  [PROPN, AUX]  1793           [US, President, U.S.]             [-]   \n",
       "IN   [ADP, SCONJ]  1744                    [of, in, to]         [About]   \n",
       "DT    [PRON, DET]  1379                   [the, a, The]            [An]   \n",
       "JJ          [ADJ]   872  [other, Russian, presidential]          [21st]   \n",
       "NNS        [NOUN]   781          [ants, troops, people]         [1970s]   \n",
       ",         [PUNCT]   699                       [,, ;, …]             [;]   \n",
       "VBD   [VERB, AUX]   658               [was, were, said]  [acknowledged]   \n",
       ".         [PUNCT]   655                       [., ?, !]             [!]   \n",
       "VBN   [VERB, AUX]   501         [been, accused, killed]             [-]   \n",
       "\n",
       "            4  \n",
       "NN   0.184520  \n",
       "NNP  0.160995  \n",
       "IN   0.156595  \n",
       "DT   0.123821  \n",
       "JJ   0.078298  \n",
       "NNS  0.070127  \n",
       ",    0.062764  \n",
       "VBD  0.059082  \n",
       ".    0.058813  \n",
       "VBN  0.044985  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92bd3b",
   "metadata": {},
   "source": [
    "## 3. N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "631592d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(doc, n, pos = False):\n",
    "    result = []\n",
    "    sentence = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            if pos:\n",
    "                sentence.append(token.tag_)\n",
    "            else:\n",
    "                sentence.append(token)\n",
    "            \n",
    "    for word in range(len(sentence) - (n-1)):\n",
    "        element = []\n",
    "        for i in range(n):\n",
    "            element.append(sentence[word+i])\n",
    "        result.append(element)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def count(ngrams):\n",
    "    counts = {}\n",
    "    for ngram in ngrams:\n",
    "        listToStr = ' '.join(map(str, ngram))\n",
    "        if listToStr in counts:\n",
    "            counts[listToStr] += 1\n",
    "        else:\n",
    "            counts[listToStr] = 1\n",
    "            \n",
    "    return counts\n",
    "\n",
    "def plot_dist(grams):\n",
    "    df = pd.DataFrame.from_dict(grams,orient=\"index\").sort_values(by=0, ascending=False)\n",
    "    df.plot(rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "553db157",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngram(doc, n=2)\n",
    "bigrams_types = ngram(doc, n=2, pos=True)\n",
    "\n",
    "bigram_counts = count(bigrams)\n",
    "bigram_counts_types = count(bigrams_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cb4601fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>of the</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in the</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to the</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "of the  82\n",
       "in the  54\n",
       "to the  43"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(bigram_counts,orient=\"index\").sort_values(by=0, ascending=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2affe7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NNP NNP</th>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT NN</th>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN DT</th>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "NNP NNP  699\n",
       "DT NN    679\n",
       "IN DT    597"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(bigram_counts_types,orient=\"index\").sort_values(by=0, ascending=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "91b64828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in response to</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Republican Party presidential</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local time UTC</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "in response to                 7\n",
       "Republican Party presidential  6\n",
       "local time UTC                 5"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = ngram(doc, n=3)\n",
    "trigrams_types = ngram(doc, n=3, pos=True)\n",
    "\n",
    "trigram_counts = count(trigrams)\n",
    "trigram_counts_types = count(trigrams_types)\n",
    "\n",
    "df = pd.DataFrame.from_dict(trigram_counts,orient=\"index\").sort_values(by=0, ascending=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "049933ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IN DT NN</th>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP NNP NNP</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT NN IN</th>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "IN DT NN     303\n",
       "NNP NNP NNP  271\n",
       "DT NN IN     219"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(trigram_counts_types,orient=\"index\").sort_values(by=0, ascending=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90655fb",
   "metadata": {},
   "source": [
    "## 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f142a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a44c8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child',\n",
       " 'be',\n",
       " 'think',\n",
       " 'to',\n",
       " 'be',\n",
       " 'age',\n",
       " 'three',\n",
       " ',',\n",
       " 'eight',\n",
       " ',',\n",
       " 'and',\n",
       " 'ten',\n",
       " 'year',\n",
       " ',',\n",
       " 'alongside',\n",
       " 'an',\n",
       " 'eighteen',\n",
       " '-',\n",
       " 'month',\n",
       " '-',\n",
       " 'old',\n",
       " 'baby',\n",
       " '.',\n",
       " 'we',\n",
       " 'mix',\n",
       " 'different',\n",
       " 'concentration',\n",
       " 'of',\n",
       " 'ROS',\n",
       " 'with',\n",
       " 'the',\n",
       " 'spore',\n",
       " ',',\n",
       " 'plate',\n",
       " 'they',\n",
       " 'out',\n",
       " 'on',\n",
       " 'petridishe',\n",
       " 'with',\n",
       " 'an',\n",
       " 'agar',\n",
       " '-',\n",
       " 'solution',\n",
       " 'where',\n",
       " 'fungus',\n",
       " 'can',\n",
       " 'grow',\n",
       " 'on',\n",
       " '.',\n",
       " 'they',\n",
       " 'feel',\n",
       " 'they',\n",
       " 'be',\n",
       " 'under',\n",
       " '-',\n",
       " 'represent',\n",
       " 'in',\n",
       " 'high',\n",
       " 'education',\n",
       " 'and',\n",
       " 'be',\n",
       " 'suffer',\n",
       " 'in',\n",
       " 'a',\n",
       " 'regional',\n",
       " 'economic',\n",
       " 'downturn',\n",
       " '.',\n",
       " 'especially',\n",
       " 'as',\n",
       " 'it',\n",
       " 'concern',\n",
       " 'a',\n",
       " 'third',\n",
       " 'party',\n",
       " 'build',\n",
       " 'up',\n",
       " 'its',\n",
       " 'military',\n",
       " 'presence',\n",
       " 'near',\n",
       " 'our',\n",
       " 'border',\n",
       " '.',\n",
       " 'Police',\n",
       " 'say',\n",
       " 'three',\n",
       " 'child',\n",
       " 'be',\n",
       " 'hospitalise',\n",
       " 'for',\n",
       " '\"',\n",
       " 'severe',\n",
       " 'dehydration',\n",
       " '\"',\n",
       " '.',\n",
       " 'Virginia',\n",
       " 'Álvarez',\n",
       " ',',\n",
       " 'who',\n",
       " 'write',\n",
       " 'the',\n",
       " 'report',\n",
       " ',',\n",
       " 'note',\n",
       " ',',\n",
       " '\"',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'their',\n",
       " 'demand',\n",
       " ',',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'start',\n",
       " 'a',\n",
       " 'dialogue',\n",
       " ',',\n",
       " 'authority',\n",
       " 'be',\n",
       " 'do',\n",
       " 'everything',\n",
       " 'they',\n",
       " 'can',\n",
       " 'to',\n",
       " 'impede',\n",
       " 'people',\n",
       " 'from',\n",
       " 'protest',\n",
       " '\"',\n",
       " '.',\n",
       " 'US',\n",
       " 'troop',\n",
       " 'and',\n",
       " 'tank',\n",
       " 'in',\n",
       " 'Poland',\n",
       " 'in',\n",
       " '2015',\n",
       " ',',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'an',\n",
       " 'early',\n",
       " 'deployment',\n",
       " 'under',\n",
       " 'Operation',\n",
       " 'Atlantic',\n",
       " 'Resolve',\n",
       " '.',\n",
       " 'furthermore',\n",
       " ',',\n",
       " 'Everson',\n",
       " 'argue',\n",
       " 'the',\n",
       " 'Fox',\n",
       " 'News',\n",
       " 'criterion',\n",
       " 'be',\n",
       " 'not',\n",
       " '\"',\n",
       " 'objective',\n",
       " ',',\n",
       " '\"',\n",
       " 'as',\n",
       " 'title',\n",
       " '11',\n",
       " 'require',\n",
       " ',',\n",
       " 'because',\n",
       " 'they',\n",
       " 'fail',\n",
       " 'to',\n",
       " 'define',\n",
       " 'the',\n",
       " 'term',\n",
       " '\"',\n",
       " 'consistently',\n",
       " '\"',\n",
       " 'and',\n",
       " '\"',\n",
       " 'recognize',\n",
       " '\"',\n",
       " 'when',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'poll',\n",
       " '.',\n",
       " 'that',\n",
       " 'decision',\n",
       " 'be',\n",
       " 'the',\n",
       " 'one',\n",
       " 'challenge',\n",
       " 'unsuccessfully',\n",
       " 'in',\n",
       " 'the',\n",
       " 'High',\n",
       " 'Court',\n",
       " '.',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'death',\n",
       " 'be',\n",
       " 'report',\n",
       " 'in',\n",
       " 'the',\n",
       " 'attack',\n",
       " 'from',\n",
       " 'either',\n",
       " 'side',\n",
       " ',',\n",
       " 'but',\n",
       " 'South',\n",
       " 'Korea',\n",
       " 'evacuate',\n",
       " 'about',\n",
       " '80',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Yeoncheon',\n",
       " 'after',\n",
       " 'at',\n",
       " 'least',\n",
       " 'one',\n",
       " 'shell',\n",
       " 'land',\n",
       " 'near',\n",
       " 'the',\n",
       " 'area',\n",
       " '.',\n",
       " 'the',\n",
       " 'nonfatal',\n",
       " 'accident',\n",
       " 'leave',\n",
       " 'an',\n",
       " 'elderly',\n",
       " 'motorcyclist',\n",
       " 'with',\n",
       " 'broken',\n",
       " 'rib',\n",
       " 'after',\n",
       " 'he',\n",
       " 'come',\n",
       " 'off',\n",
       " 'his',\n",
       " 'bike',\n",
       " 'avoid',\n",
       " 'the',\n",
       " '170',\n",
       " 'tonne',\n",
       " 'of',\n",
       " 'fall',\n",
       " 'debris',\n",
       " '.',\n",
       " 'one',\n",
       " 'driver',\n",
       " 'be',\n",
       " 'accuse',\n",
       " 'of',\n",
       " 'move',\n",
       " '34',\n",
       " 'people',\n",
       " ',',\n",
       " 'ten',\n",
       " 'of',\n",
       " 'they',\n",
       " 'child',\n",
       " ',',\n",
       " 'into',\n",
       " 'Austria',\n",
       " 'from',\n",
       " 'Serbia',\n",
       " '.',\n",
       " 'pavlensky',\n",
       " ',',\n",
       " 'Oksana',\n",
       " ',',\n",
       " 'and',\n",
       " 'their',\n",
       " 'two',\n",
       " 'daughter',\n",
       " 'leave',\n",
       " 'for',\n",
       " 'Ukraine',\n",
       " 'in',\n",
       " 'mid',\n",
       " '-',\n",
       " 'December',\n",
       " 'before',\n",
       " 'fly',\n",
       " 'to',\n",
       " 'France',\n",
       " '.',\n",
       " 'their',\n",
       " 'detention',\n",
       " 'relate',\n",
       " 'to',\n",
       " 'Thursday',\n",
       " \"'s\",\n",
       " 'discovery',\n",
       " 'of',\n",
       " 'a',\n",
       " 'lorry',\n",
       " 'full',\n",
       " 'of',\n",
       " 'corpse',\n",
       " 'in',\n",
       " 'Austria',\n",
       " '.',\n",
       " 'Robben',\n",
       " 'sign',\n",
       " 'contract',\n",
       " 'extension',\n",
       " 'with',\n",
       " 'bayern',\n",
       " 'german',\n",
       " 'football',\n",
       " 'club',\n",
       " 'FC',\n",
       " 'Bayern',\n",
       " 'Munich',\n",
       " 'yesterday',\n",
       " 'announce',\n",
       " 'dutch',\n",
       " 'winger',\n",
       " 'Arjen',\n",
       " 'Robben',\n",
       " 'have',\n",
       " 'sign',\n",
       " 'a',\n",
       " 'one',\n",
       " 'contract',\n",
       " 'extension',\n",
       " 'until',\n",
       " 'June',\n",
       " '2018',\n",
       " '.',\n",
       " 'Anastasia',\n",
       " 'Slonina',\n",
       " ',',\n",
       " 'an',\n",
       " 'actress',\n",
       " 'work',\n",
       " 'in',\n",
       " 'a',\n",
       " 'theatre',\n",
       " 'in',\n",
       " 'Moscow',\n",
       " 'call',\n",
       " 'Teatr.doc',\n",
       " ',',\n",
       " 'know',\n",
       " 'for',\n",
       " 'liberal',\n",
       " 'play',\n",
       " ',',\n",
       " 'file',\n",
       " 'rape',\n",
       " 'charge',\n",
       " 'on',\n",
       " 'Pavlensky',\n",
       " 'and',\n",
       " 'his',\n",
       " 'partner',\n",
       " 'Oksana',\n",
       " '.',\n",
       " 'Bos',\n",
       " 'obtain',\n",
       " 'his',\n",
       " 'doctorate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Copenhagen',\n",
       " '.',\n",
       " 'criminal',\n",
       " 'suspect',\n",
       " 'in',\n",
       " 'Hungary',\n",
       " 'can',\n",
       " 'generally',\n",
       " 'be',\n",
       " 'hold',\n",
       " 'for',\n",
       " '72',\n",
       " 'hour',\n",
       " 'before',\n",
       " 'charge',\n",
       " 'but',\n",
       " 'prosecutor',\n",
       " ',',\n",
       " 'cite',\n",
       " 'the',\n",
       " 'seriousness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'case',\n",
       " ',',\n",
       " 'want',\n",
       " 'the',\n",
       " 'period',\n",
       " 'extend',\n",
       " '.',\n",
       " 'these',\n",
       " 'action',\n",
       " 'threaten',\n",
       " 'our',\n",
       " 'interest',\n",
       " ',',\n",
       " 'our',\n",
       " 'security',\n",
       " '.',\n",
       " 'wwikinew',\n",
       " ':',\n",
       " 'what',\n",
       " 'method',\n",
       " 'and',\n",
       " 'equipment',\n",
       " 'be',\n",
       " 'use',\n",
       " 'for',\n",
       " 'this',\n",
       " 'investigation',\n",
       " '?',\n",
       " 'Bianchi',\n",
       " \"'s\",\n",
       " 'father',\n",
       " ',',\n",
       " 'Philippe',\n",
       " ',',\n",
       " 'describe',\n",
       " 'his',\n",
       " 'non',\n",
       " '-',\n",
       " 'progress',\n",
       " 'as',\n",
       " 'a',\n",
       " '\"',\n",
       " 'daily',\n",
       " 'torture',\n",
       " '\"',\n",
       " '.',\n",
       " 'next',\n",
       " 'week',\n",
       " ',',\n",
       " 'on',\n",
       " 'January',\n",
       " '24',\n",
       " ',',\n",
       " 'the',\n",
       " 'Supreme',\n",
       " 'Court',\n",
       " 'be',\n",
       " 'due',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'a',\n",
       " 'decision',\n",
       " 'in',\n",
       " 'a',\n",
       " 'case',\n",
       " 'challenge',\n",
       " 'the',\n",
       " 'government',\n",
       " \"'s\",\n",
       " 'right',\n",
       " 'to',\n",
       " 'issue',\n",
       " 'article',\n",
       " '50',\n",
       " '—',\n",
       " 'which',\n",
       " 'start',\n",
       " 'the',\n",
       " 'Brexit',\n",
       " 'negotiation',\n",
       " '—',\n",
       " 'without',\n",
       " 'the',\n",
       " 'consultation',\n",
       " 'of',\n",
       " 'Parliament',\n",
       " '.',\n",
       " 'furthermore',\n",
       " ',',\n",
       " 'the',\n",
       " 'datum',\n",
       " 'of',\n",
       " 'radar',\n",
       " 'at',\n",
       " 'Maldives',\n",
       " 'airport',\n",
       " 'have',\n",
       " 'also',\n",
       " 'be',\n",
       " 'analyse',\n",
       " 'and',\n",
       " 'show',\n",
       " 'no',\n",
       " 'indication',\n",
       " 'of',\n",
       " 'the',\n",
       " 'say',\n",
       " 'flight',\n",
       " '\"',\n",
       " ',',\n",
       " 'say',\n",
       " 'malaysian',\n",
       " 'Transport',\n",
       " 'Minister',\n",
       " 'Hishamuddin',\n",
       " 'Hussein',\n",
       " '.',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'dangerous',\n",
       " 'driving',\n",
       " 'and',\n",
       " 'cause',\n",
       " 'serious',\n",
       " 'injury',\n",
       " 'by',\n",
       " 'dangerous',\n",
       " 'driving',\n",
       " 'he',\n",
       " 'also',\n",
       " 'face',\n",
       " 'three',\n",
       " 'count',\n",
       " 'of',\n",
       " 'criminal',\n",
       " 'damage',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'bridge',\n",
       " 'and',\n",
       " 'damaged',\n",
       " 'vehicle',\n",
       " '.',\n",
       " 'all',\n",
       " '239',\n",
       " 'passenger',\n",
       " 'and',\n",
       " 'crew',\n",
       " 'be',\n",
       " 'believe',\n",
       " 'to',\n",
       " 'be',\n",
       " 'dead',\n",
       " '.',\n",
       " 'Bianchi',\n",
       " \"'s\",\n",
       " 'helmet',\n",
       " 'became',\n",
       " 'wedge',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'tractor',\n",
       " ',',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'diffuse',\n",
       " 'axonal',\n",
       " 'injury',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'fall',\n",
       " 'into',\n",
       " 'a',\n",
       " 'coma',\n",
       " '.',\n",
       " 'Wikinews',\n",
       " ':',\n",
       " 'be',\n",
       " 'there',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'this',\n",
       " 'research',\n",
       " 'up',\n",
       " '?',\n",
       " 'it',\n",
       " 'be',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'extrapolate',\n",
       " 'to',\n",
       " 'other',\n",
       " 'ant',\n",
       " ',',\n",
       " 'as',\n",
       " 'different',\n",
       " 'specie',\n",
       " 'lead',\n",
       " 'very',\n",
       " 'different',\n",
       " 'life',\n",
       " '.',\n",
       " 'republican',\n",
       " 'Senators',\n",
       " 'set',\n",
       " 'a',\n",
       " 'date',\n",
       " 'of',\n",
       " 'January',\n",
       " '27',\n",
       " 'to',\n",
       " 'repeal',\n",
       " 'Obamacare',\n",
       " ',',\n",
       " 'accord',\n",
       " 'to',\n",
       " 'NBC',\n",
       " 'News',\n",
       " '.',\n",
       " 'last',\n",
       " 'month',\n",
       " 'the',\n",
       " 'US',\n",
       " 'expel',\n",
       " '35',\n",
       " 'russian',\n",
       " 'diplomat',\n",
       " 'accuse',\n",
       " 'of',\n",
       " 'espionage',\n",
       " ',',\n",
       " 'and',\n",
       " 'sanction',\n",
       " 'russian',\n",
       " 'intelligence',\n",
       " 'agency',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'he',\n",
       " 'believe',\n",
       " 'Everson',\n",
       " 'only',\n",
       " 'have',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'success',\n",
       " 'if',\n",
       " 'he',\n",
       " 'actually',\n",
       " 'file',\n",
       " 'a',\n",
       " 'lawsuit',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'simply',\n",
       " 'complain',\n",
       " 'to',\n",
       " 'the',\n",
       " 'FEC',\n",
       " '.',\n",
       " 'due',\n",
       " 'to',\n",
       " 'his',\n",
       " 'severe',\n",
       " 'deterioration',\n",
       " ',',\n",
       " 'Israel',\n",
       " 'be',\n",
       " 'force',\n",
       " 'to',\n",
       " 'hospitalize',\n",
       " 'he',\n",
       " ',',\n",
       " 'and',\n",
       " 'place',\n",
       " 'he',\n",
       " 'in',\n",
       " 'Assaf',\n",
       " 'Harofe',\n",
       " 'hospital',\n",
       " 'in',\n",
       " 'Israel',\n",
       " '.',\n",
       " 'a',\n",
       " 'team',\n",
       " 'be',\n",
       " 'duly',\n",
       " 'dispatch',\n",
       " 'and',\n",
       " 'prepare',\n",
       " 'a',\n",
       " 'report',\n",
       " 'with',\n",
       " 'the',\n",
       " 'intent',\n",
       " 'to',\n",
       " 'use',\n",
       " 'it',\n",
       " 'for',\n",
       " 'assist',\n",
       " 'the',\n",
       " 'victim',\n",
       " \"'\",\n",
       " 'family',\n",
       " '.',\n",
       " 'one',\n",
       " 'set',\n",
       " 'stimulate',\n",
       " 'prey',\n",
       " 'pursuit',\n",
       " 'behavior',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'stalk',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'other',\n",
       " 'stimulate',\n",
       " 'the',\n",
       " 'animal',\n",
       " 'to',\n",
       " 'use',\n",
       " 'its',\n",
       " 'jaw',\n",
       " 'and',\n",
       " 'neck',\n",
       " 'muscle',\n",
       " '.',\n",
       " 'May',\n",
       " \"'s\",\n",
       " 'speech',\n",
       " 'stress',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'friendliness',\n",
       " 'between',\n",
       " 'Britain',\n",
       " 'and',\n",
       " 'the',\n",
       " 'EU',\n",
       " 'but',\n",
       " 'also',\n",
       " 'rule',\n",
       " 'out',\n",
       " 'any',\n",
       " 'deal',\n",
       " 'that',\n",
       " 'do',\n",
       " 'not',\n",
       " 'allow',\n",
       " 'the',\n",
       " 'british',\n",
       " 'government',\n",
       " 'to',\n",
       " 'control',\n",
       " 'immigration',\n",
       " ',',\n",
       " 'or',\n",
       " 'which',\n",
       " 'would',\n",
       " 'require',\n",
       " 'Britain',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'be',\n",
       " 'bind',\n",
       " 'by',\n",
       " 'decision',\n",
       " 'of',\n",
       " 'the',\n",
       " 'European',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Justice',\n",
       " '.',\n",
       " 'we',\n",
       " 'be',\n",
       " 'work',\n",
       " 'with',\n",
       " 'legislative',\n",
       " 'leader',\n",
       " 'at',\n",
       " 'this',\n",
       " 'very',\n",
       " 'moment',\n",
       " 'to',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'craft',\n",
       " 'legislation',\n",
       " 'that',\n",
       " 'will',\n",
       " 'repeal',\n",
       " 'the',\n",
       " 'most',\n",
       " 'corrosive',\n",
       " 'element',\n",
       " 'of',\n",
       " 'Obamacare',\n",
       " '—',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'mandate',\n",
       " ',',\n",
       " 'the',\n",
       " 'taxis',\n",
       " ',',\n",
       " 'the',\n",
       " 'penalty',\n",
       " '—',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " ',',\n",
       " 'move',\n",
       " 'separate',\n",
       " 'legislation',\n",
       " 'that',\n",
       " 'will',\n",
       " 'allow',\n",
       " 'we',\n",
       " 'to',\n",
       " 'introduce',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'reform',\n",
       " 'in',\n",
       " 'american',\n",
       " 'health',\n",
       " 'care',\n",
       " 'that',\n",
       " \"'ll\",\n",
       " 'lower',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'health',\n",
       " 'insurance',\n",
       " 'without',\n",
       " 'grow',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'government',\n",
       " ',',\n",
       " '\"',\n",
       " 'say',\n",
       " 'Vice',\n",
       " 'President',\n",
       " '-',\n",
       " 'elect',\n",
       " 'Mike',\n",
       " 'Pence',\n",
       " '.',\n",
       " 'the',\n",
       " 'JAS',\n",
       " '39c',\n",
       " 'Gripen',\n",
       " 'crash',\n",
       " 'onto',\n",
       " 'a',\n",
       " 'runway',\n",
       " 'at',\n",
       " 'around',\n",
       " '9:30',\n",
       " 'am',\n",
       " 'local',\n",
       " 'time',\n",
       " '(',\n",
       " '0230',\n",
       " 'UTC',\n",
       " ')',\n",
       " 'and',\n",
       " 'explode',\n",
       " ',',\n",
       " 'close',\n",
       " 'the',\n",
       " 'airport',\n",
       " 'to',\n",
       " 'commercial',\n",
       " 'flight',\n",
       " '.',\n",
       " 'after',\n",
       " 'the',\n",
       " 'service',\n",
       " ',',\n",
       " 'Black',\n",
       " 'be',\n",
       " 'to',\n",
       " 'be',\n",
       " 'inter',\n",
       " 'in',\n",
       " 'a',\n",
       " 'private',\n",
       " 'ceremony',\n",
       " ',',\n",
       " 'next',\n",
       " 'to',\n",
       " 'the',\n",
       " 'grave',\n",
       " 'of',\n",
       " 'her',\n",
       " 'parent',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Allerton',\n",
       " 'cemetery',\n",
       " '.',\n",
       " 'other',\n",
       " 'planned',\n",
       " 'host',\n",
       " 'nation',\n",
       " 'be',\n",
       " 'Estonia',\n",
       " ',',\n",
       " 'Latvia',\n",
       " ',',\n",
       " 'Lithuania',\n",
       " ',',\n",
       " 'Romania',\n",
       " ',',\n",
       " 'Bulgaria',\n",
       " ',',\n",
       " 'and',\n",
       " 'Hungary',\n",
       " '.',\n",
       " 'Tomasz',\n",
       " 'Szatkowski',\n",
       " ',',\n",
       " 'Polish',\n",
       " 'Undersecretary',\n",
       " 'of',\n",
       " 'State',\n",
       " 'for',\n",
       " 'Defence',\n",
       " ',',\n",
       " 'note',\n",
       " '\"',\n",
       " 'large',\n",
       " 'exercise',\n",
       " '\"',\n",
       " 'by',\n",
       " 'Russia',\n",
       " 'near',\n",
       " 'the',\n",
       " 'polish',\n",
       " 'border',\n",
       " 'as',\n",
       " 'another',\n",
       " 'reason',\n",
       " 'the',\n",
       " 'troop',\n",
       " 'be',\n",
       " 'need',\n",
       " '.',\n",
       " 'wikinews',\n",
       " ':',\n",
       " 'be',\n",
       " 'there',\n",
       " 'any',\n",
       " 'ethical',\n",
       " 'consideration',\n",
       " 'around',\n",
       " 'expose',\n",
       " 'ant',\n",
       " 'to',\n",
       " 'toxin',\n",
       " 'and',\n",
       " 'parasite',\n",
       " '?',\n",
       " 'his',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'stint',\n",
       " 'behind',\n",
       " 'bar',\n",
       " 'come',\n",
       " 'last',\n",
       " 'month',\n",
       " ',',\n",
       " 'when',\n",
       " 'he',\n",
       " 'be',\n",
       " 'arrest',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b30da199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children are thought to be aged three , eight , and ten years , alongside an eighteen-month-old baby ."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list(doc.sents)\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba2bd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = {}\n",
    "lemma_set = []\n",
    "token_set = []\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "for i in range(len(sentences)):\n",
    "    for token in sentences[i]:\n",
    "        if not token.is_punct:\n",
    "            if token.lemma_ not in lemmas.keys():\n",
    "                lemmas[token.lemma_] = {}\n",
    "                lemmas[token.lemma_][token.text] = [i]\n",
    "                lemma_set.append(token.lemma_)\n",
    "                token_set.append(token.text)\n",
    "            elif token.text not in lemmas[token.lemma_].keys():\n",
    "                lemmas[token.lemma_][token.text] = [i]\n",
    "                token_set.append(token.text)\n",
    "            elif token.text in lemmas[token.lemma_].keys() and token.lemma_ in lemmas.keys():\n",
    "                lemmas[token.lemma_][token.text] += [i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "066512b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "say\n",
      "report\n",
      "start\n",
      "do\n",
      "early\n",
      "challenge\n",
      "fall\n",
      "move\n",
      "fly\n",
      "call\n",
      "use\n",
      "close\n",
      "appear\n",
      "find\n",
      "go\n",
      "seek\n",
      "kill\n",
      "measure\n",
      "claim\n",
      "result\n",
      "man\n",
      "include\n",
      "give\n",
      "respond\n",
      "name\n",
      "see\n",
      "add\n",
      "make\n",
      "run\n",
      "take\n",
      "remain\n",
      "election\n",
      "average\n"
     ]
    }
   ],
   "source": [
    "for k in lemmas.keys():\n",
    "    if len(lemmas[k].keys()) > 3:\n",
    "        print(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dbb416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['challenged', 'challenging', 'challenges', 'challenge'])\n",
      "\n",
      "That decision was the one challenged unsuccessfully in the High Court .\n",
      "\n",
      "Next week , on January 24 , the Supreme Court is due to deliver a decision in a case challenging the government 's right to issue Article 50 — which starts the Brexit negotiations — without the consultation of Parliament .\n",
      "\n",
      "U.S. presidential candidate Mark Everson challenges debate exclusion U.S. Republican Party presidential candidate Mark Everson , former commissioner of the Internal Revenue Service ( IRS ) , filed a complaint on Monday with the Federal Election Commission ( FEC ) to challenge his exclusion from Thursday 's first Fox News Republican Party presidential debate .\n",
      "\n",
      "Election law expert Richard Winger , publisher of Ballot Access News , says Everson is \" completely correct \" in his challenge .\n"
     ]
    }
   ],
   "source": [
    "print(lemmas['challenge'].keys())\n",
    "print()\n",
    "lem1tok1 = lemmas['challenge']['challenged'][0]\n",
    "lem1tok2 = lemmas['challenge']['challenging'][0]\n",
    "lem1tok3 = lemmas['challenge']['challenges'][0]\n",
    "lem1tok4 = lemmas['challenge']['challenge'][1]\n",
    "print(sentences[lem1tok1])\n",
    "print()\n",
    "print(sentences[lem1tok2])\n",
    "print()\n",
    "print(sentences[lem1tok3])\n",
    "print()\n",
    "print(sentences[lem1tok4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cf509",
   "metadata": {},
   "source": [
    "### Part 5. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f28e46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence object and an array for entity information\n",
    "doc_sent = doc.sents\n",
    "array = np.zeros(len(list(doc.sents))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "840b6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over sentences save entities in array\n",
    "counter = 0\n",
    "first_five = []\n",
    "for sent in doc.sents:\n",
    "    first_five.append(sent)\n",
    "    temp_entity = []\n",
    "    sent_text = nlp(str(sent))\n",
    "    \n",
    "    for ent in sent_text.ents:\n",
    "        temp_entity.append([ent.text, ent.label_])\n",
    "    \n",
    "    array[counter] = temp_entity\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c129e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of named entities: 2108\n",
      "Number of different entity labels: 17\n"
     ]
    }
   ],
   "source": [
    "# Get entities and entity labels back from array\n",
    "named_entities = 0\n",
    "entity_labels = []\n",
    "\n",
    "for i in range(len(array)):\n",
    "    \n",
    "    temp_array = np.array(array[i]).T\n",
    "    \n",
    "    if temp_array.size != 0:\n",
    "        named_entities += len(temp_array[0]) + 1\n",
    "        entity_labels.append(temp_array[1].tolist())\n",
    "    \n",
    "print(f\"Number of named entities: {named_entities}\")    \n",
    "print(f\"Number of different entity labels: {len(set(np.hstack(entity_labels).tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5bc3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "children are thought to be aged three , eight , and ten years , alongside an eighteen-month-old baby .\n",
      "Entities: ['three , eight' 'ten years' 'eighteen-month-old']\n",
      "Labels: ['DATE' 'DATE' 'DATE']\n",
      "\n",
      "Sentence 2:\n",
      "We mixed different concentrations of ROS with the spores , plated them out on petridishes with an agar-solution where fungus can grow on .\n",
      "Entities: ['ROS']\n",
      "Labels: ['GPE']\n",
      "\n",
      "Sentence 3:\n",
      "They feel they are under-represented in higher education and are suffering in a regional economic downturn .\n",
      "\n",
      "Sentence 4:\n",
      "Especially as it concerns a third party building up its military presence near our borders .\n",
      "Entities: ['third']\n",
      "Labels: ['ORDINAL']\n",
      "\n",
      "Sentence 5:\n",
      "Police said three children were hospitalised for \" severe dehydration \" .\n",
      "Entities: ['three']\n",
      "Labels: ['CARDINAL']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse first five sentences\n",
    "for i in range(0,5):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(first_five[i])\n",
    "    if np.array(array[i]).T.size != 0:\n",
    "        print(f\"Entities: {np.array(array[i]).T[0]}\")\n",
    "        print(f\"Labels: {np.array(array[i]).T[1]}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c78bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
